{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNdH9WWEDsW-"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJXb82K-DkZi"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq wandb pytorch-lightning==1.9.3 torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njv8plSDDqeL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchsummary import summary\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WfewXt7DwsN",
        "outputId": "76994b41-a7d5-419c-c110-bc6f8580ca23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmyntiuk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "pl.seed_everything(42)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "import wandb\n",
        "\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "import os\n",
        "from typing import Any, Dict, cast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
        "from torchmetrics import Metric, MetricCollection\n",
        "from torchmetrics.classification import (\n",
        "    MulticlassAccuracy,\n",
        "    MulticlassFBetaScore,\n",
        "    AUROC,\n",
        "    BinaryROC,\n",
        "    Recall,\n",
        "    Specificity,\n",
        ")\n",
        "\n",
        "from torchmetrics.classification.precision_recall_curve import BinaryPrecisionRecallCurve\n",
        "from torchmetrics.utilities.data import dim_zero_cat\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4CR3CPUJziX"
      },
      "outputs": [],
      "source": [
        "! pip install -qqq timm torchgeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3KEo-GBJ39w"
      },
      "outputs": [],
      "source": [
        "from torchgeo.datasets import unbind_samples\n",
        "from torchgeo.models import get_weight\n",
        "from torchgeo.trainers import utils\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyPemxdHDxUk",
        "outputId": "5c06c4f6-5380-4319-f27d-d7e55bfec382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euu-imXDDzFu"
      },
      "outputs": [],
      "source": [
        "! cp /content/drive/MyDrive/minefree-class-128.zip /content/\n",
        "! unzip -q /content/minefree-class-128.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idAP84bM1FIt"
      },
      "outputs": [],
      "source": [
        "! mv /content/minefree-class-128/train/bombed /content/minefree-class-128/train/1bombed\n",
        "! mv /content/minefree-class-128/val/bombed /content/minefree-class-128/val/1bombed\n",
        "! mv /content/minefree-class-128/train/not-bombed /content/minefree-class-128/train/0not-bombed\n",
        "! mv /content/minefree-class-128/val/not-bombed /content/minefree-class-128/val/0not-bombed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD0edHcIEk3w"
      },
      "source": [
        "## Define the sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH0IkyslD4WY"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"random\", # grid, random\n",
        "    \"metric\": {\n",
        "      \"name\": \"accuracy\",\n",
        "      \"goal\": \"maximize\"   \n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\n",
        "            \"values\": [100]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0., 0.2]\n",
        "        },\n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.00005, 0.0005]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [1e-4, 1e-5]\n",
        "        },\n",
        "        \"lr_scheduler\": {\n",
        "            \"values\": [\"on_plateau\"]\n",
        "        },\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\n",
        "                \"adamw\",\n",
        "                \"sgd\"\n",
        "            ]\n",
        "        },\n",
        "        \"model_name\": {\n",
        "            \"values\": [\n",
        "                \"resnet50\",\n",
        "                \"vit_small_patch16_224\"\n",
        "            ]\n",
        "        },\n",
        "        \"weights\": {\n",
        "            \"values\": [\n",
        "                \"imagenet\",\n",
        "                \"sentinel2\"\n",
        "            ]\n",
        "        },\n",
        "        \"num_layers_to_finetune\": {\n",
        "            \"values\": [None]\n",
        "        },\n",
        "        \"learning_rate_schedule_patience\": {\n",
        "            \"values\": [5, 3]\n",
        "        },\n",
        "        \"early_stop_patience\": {\n",
        "            \"values\": [10],\n",
        "        },\n",
        "        \"normalize\": {\n",
        "            \"values\": [False, True]\n",
        "        },\n",
        "        \"base_size\": {\n",
        "            \"values\": [64, 128]\n",
        "        },\n",
        "        \"dro\": {\n",
        "            \"values\": [\"up\", None]\n",
        "        }\n",
        "        \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9YKjPb0Fe1n"
      },
      "source": [
        "### DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3leFcivGAHvr"
      },
      "outputs": [],
      "source": [
        "def compute_data_stats(data_path, transform, seed=0):\n",
        "    unnormalized_image_data = ImageFolder(\n",
        "        root=data_path, transform=transforms.Compose(transform)\n",
        "    )\n",
        "    # Normalize data using full data stats. It's a bit of a leakage.\n",
        "    initial_loader = DataLoader(\n",
        "        unnormalized_image_data,\n",
        "        batch_size=len(unnormalized_image_data),\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    images, labels = next(iter(initial_loader))\n",
        "\n",
        "    # shape of images = [b,c,w,h]\n",
        "    mean, std = images.mean([0, 2, 3]), images.std([0, 2, 3])\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2b98MkDFd4z"
      },
      "outputs": [],
      "source": [
        "class DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=32, val_size=0.1, normalize=False, dro=False, base_size=64, num_workers=2):\n",
        "      super().__init__()\n",
        "      self.base_size = base_size\n",
        "      self.dro = dro\n",
        "      self.val_size = val_size\n",
        "      self.data_dir = data_dir\n",
        "      self.batch_size = batch_size\n",
        "      self.num_workers = num_workers\n",
        "\n",
        "      self.train_transforms_list = [\n",
        "        transforms.Resize((self.base_size, self.base_size)),\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "      ]\n",
        "      self.test_transforms_list = [\n",
        "          transforms.Resize((self.base_size, self.base_size)),\n",
        "          transforms.ToTensor(),\n",
        "      ]\n",
        "      if normalize:\n",
        "        train_mean, train_std = compute_data_stats(\n",
        "            os.path.join(self.data_dir, \"train\"),\n",
        "            self.train_transforms_list,\n",
        "        )\n",
        "        self.train_transforms_list.append(transforms.Normalize(train_mean, train_std))\n",
        "        self.test_transforms_list.append(transforms.Normalize(train_mean, train_std))\n",
        "      self.train_transforms = transforms.Compose(self.train_transforms_list)\n",
        "      self.test_transforms = transforms.Compose(self.test_transforms_list)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "      if stage in [\"fit\", \"validate\"] or stage is None:\n",
        "        data = ImageFolder(os.path.join(self.data_dir, \"train\"),)\n",
        "        n_val = int(np.floor(self.val_size * len(data)))\n",
        "        self.train, self.validate = random_split(data, [len(data) - n_val, n_val])\n",
        "        self.train.dataset.transform = self.train_transforms\n",
        "        self.validate.dataset.transform = self.test_transforms\n",
        "\n",
        "      if stage == \"test\" or stage is None:\n",
        "        self.test = ImageFolder(os.path.join(self.data_dir, \"val\"),\n",
        "                                self.test_transforms,)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "      if self.dro == \"up\":  # Upsampling the minority class.\n",
        "        labels = []\n",
        "        for i in range(len(self.train)):\n",
        "            item = self.train[i]\n",
        "            labels.append(item[1])\n",
        "        labels = np.array(labels)\n",
        "        sample_weights = np.ones_like(labels)\n",
        "        pos_label = 1\n",
        "        neg_label = 0\n",
        "        pos_prop = (labels == pos_label).mean()\n",
        "        neg_prop = (labels == neg_label).mean()\n",
        "        for i in range(len(labels)):\n",
        "            if labels[i] == pos_label:\n",
        "                sample_weights[i] /= pos_prop\n",
        "            elif labels[i] == neg_label:\n",
        "                sample_weights[i] /= neg_prop\n",
        "\n",
        "        sampler = torch.utils.data.WeightedRandomSampler(\n",
        "            sample_weights,\n",
        "            num_samples=self.batch_size,\n",
        "            replacement=True,\n",
        "        )\n",
        "        train = DataLoader(self.train,\n",
        "                           batch_size=self.batch_size,\n",
        "                           sampler=sampler,\n",
        "                           num_workers=self.num_workers,\n",
        "                           )\n",
        "      else:\n",
        "        train = DataLoader(self.train,\n",
        "                           batch_size=self.batch_size,\n",
        "                           num_workers=self.num_workers,\n",
        "                           )\n",
        "      return train\n",
        "\n",
        "    def val_dataloader(self):\n",
        "      val = DataLoader(self.validate,\n",
        "                       batch_size=self.batch_size,\n",
        "                       num_workers=self.num_workers,\n",
        "                       )\n",
        "      return val\n",
        "\n",
        "    def test_dataloader(self):\n",
        "      test = DataLoader(self.test,\n",
        "                        batch_size=self.batch_size,\n",
        "                        num_workers=self.num_workers,\n",
        "                        )\n",
        "      return test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BArKRD7bYhk"
      },
      "source": [
        "### Metric - TPR @ fixed FPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXey7Ew0bYHi"
      },
      "outputs": [],
      "source": [
        "class TPR_at_FPR(BinaryPrecisionRecallCurve):\n",
        "  def __init__(\n",
        "    self,\n",
        "    max_fpr,\n",
        "    thresholds = None,\n",
        "    ignore_index = None,\n",
        "    **kwargs: Any,\n",
        "  ) -> None:\n",
        "    super().__init__(thresholds, ignore_index, validate_args=False, **kwargs)\n",
        "    self.max_fpr = max_fpr\n",
        "\n",
        "  def _compute_TPR_at_FPR(self, max_fpr, pred, target):\n",
        "    \"\"\"Return maximal possible TPR and the best threshold for the maximal FPR.\"\"\"\n",
        "    roc = BinaryROC(task=\"binary\")\n",
        "    fprs, tprs, thresholds = roc(pred, target)\n",
        "    try:\n",
        "      _, max_tpr, best_threshold = max(\n",
        "          ((fpr, tpr, tresh) for fpr, tpr, tresh in zip(fprs, tprs, thresholds) if fpr <= max_fpr),\n",
        "           key=lambda t: t[1]\n",
        "      )\n",
        "    except ValueError:\n",
        "      max_tpr = torch.tensor(0.0, device=fprs.device, dtype=fprs.dtype)\n",
        "      best_threshold = torch.tensor(0)\n",
        "    if max_tpr == 0.0:\n",
        "      best_threshold = torch.tensor(1e6, device=thresholds.device, dtype=thresholds.dtype)\n",
        "\n",
        "    return max_tpr\n",
        "\n",
        "  def compute(self):\n",
        "    return self._compute_TPR_at_FPR(self.max_fpr, dim_zero_cat(self.preds), dim_zero_cat(self.target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B965p9PNFso2"
      },
      "source": [
        "### ClassificationTask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lf-7qN3Fv07"
      },
      "outputs": [],
      "source": [
        "class ClassificationTask(pl.LightningModule):\n",
        "    \"\"\"LightningModule for image classification.\n",
        "    Supports any available `Timm model\n",
        "    <https://rwightman.github.io/pytorch-image-models/>`_\n",
        "    as an architecture choice. To see a list of available\n",
        "    models, you can do:\n",
        "    .. code-block:: python\n",
        "        import timm\n",
        "        print(timm.list_models())\n",
        "    \"\"\"\n",
        "\n",
        "    def config_model(self) -> None:\n",
        "        \"\"\"Configures the model based on kwargs parameters passed to the constructor.\"\"\"\n",
        "       \n",
        "        # Create model\n",
        "        weights = self.hyperparams[\"weights\"]\n",
        "        imagenet_pretrained = weights == \"imagenet\"\n",
        "        self.model = timm.create_model(\n",
        "            self.hyperparams[\"model\"],\n",
        "            num_classes=self.hyperparams[\"num_classes\"],\n",
        "            in_chans=self.hyperparams[\"in_channels\"],\n",
        "            drop_rate=self.hyperparams[\"dropout\"],\n",
        "            pretrained=imagenet_pretrained,\n",
        "        )\n",
        "\n",
        "        # Load weights\n",
        "        try:\n",
        "            if not imagenet_pretrained:\n",
        "                state_dict = get_weight(weights).get_state_dict(progress=True)\n",
        "                self.model = utils.load_state_dict(self.model, state_dict)\n",
        "                print(f\"Loaded {weights} successfully.\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        num_layers_to_finetune = self.hyperparams[\"num_layers_to_finetune\"]\n",
        "        if num_layers_to_finetune is not None:\n",
        "            for parameter in list(self.model.parameters())[:-num_layers_to_finetune]:\n",
        "                parameter.requires_grad = False\n",
        "\n",
        "    def config_task(self) -> None:\n",
        "        \"\"\"Configures the task based on kwargs parameters passed to the constructor.\"\"\"\n",
        "        self.config_model()\n",
        "\n",
        "        if self.hyperparams[\"loss\"] == \"ce\":\n",
        "            self.loss: nn.Module = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            raise ValueError(f\"Loss type '{self.hyperparams['loss']}' is not valid.\")\n",
        "\n",
        "    def __init__(self, **kwargs: Any) -> None:\n",
        "        \"\"\"Initialize the LightningModule with a model and loss function.\n",
        "        Keyword Args:\n",
        "            model: Name of the classification model use\n",
        "            loss: Name of the loss function, accepts \"ce\", \"jaccard\", or \"focal\"\n",
        "            weights: Either a weight enum, the string representation of a weight enum,\n",
        "                True for ImageNet weights, False or None for random weights,\n",
        "                or the path to a saved model state dict.\n",
        "            num_classes: Number of prediction classes\n",
        "            in_channels: Number of input channels to model\n",
        "            learning_rate: Learning rate for optimizer\n",
        "            learning_rate_schedule_patience: Patience for learning rate scheduler\n",
        "        .. versionchanged:: 0.4\n",
        "           The *classification_model* parameter was renamed to *model*.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Creates `self.hparams` from kwargs\n",
        "        self.save_hyperparameters()\n",
        "        self.hyperparams = cast(Dict[str, Any], self.hparams)\n",
        "\n",
        "        self.config_task()\n",
        "\n",
        "        self.train_metrics = MetricCollection(\n",
        "            {\n",
        "              \"AUROC\": AUROC(\n",
        "                  task=\"binary\",\n",
        "                  num_classes=self.hyperparams[\"num_classes\"],\n",
        "              ),\n",
        "              \"TPR\": Recall(\n",
        "                  task=\"binary\",\n",
        "                  average=\"macro\",\n",
        "                  num_classes=self.hyperparams[\"num_classes\"],\n",
        "              ),\n",
        "              \"TNR\": Specificity(\n",
        "                  task=\"binary\",\n",
        "                  average=\"macro\",\n",
        "                  num_classes=self.hyperparams[\"num_classes\"],\n",
        "              ),\n",
        "              \"TPR@FPR=0_2\": TPR_at_FPR(\n",
        "                  max_fpr=0.2,\n",
        "              ),\n",
        "              \"TPR@FPR=0_1\": TPR_at_FPR(\n",
        "                  max_fpr=0.1,\n",
        "              ),\n",
        "              \"TPR@FPR=0_05\": TPR_at_FPR(\n",
        "                  max_fpr=0.05,\n",
        "              ),\n",
        "\n",
        "            },\n",
        "            prefix=\"train_\",\n",
        "        )\n",
        "        self.val_metrics = self.train_metrics.clone(prefix=\"val_\")\n",
        "        self.test_metrics = self.train_metrics.clone(prefix=\"test_\")\n",
        "\n",
        "    def forward(self, *args: Any, **kwargs: Any) -> Any:\n",
        "        \"\"\"Forward pass of the model.\n",
        "        Args:\n",
        "            x: input image\n",
        "        Returns:\n",
        "            prediction\n",
        "        \"\"\"\n",
        "        return self.model(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, *args: Any, **kwargs: Any) -> Tensor:\n",
        "        \"\"\"Compute and return the training loss.\n",
        "        Args:\n",
        "            batch: the output of your DataLoader\n",
        "        Returns:\n",
        "            training loss\n",
        "        \"\"\"\n",
        "        batch = args[0]\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "        # by default, the train step logs every `log_every_n_steps` steps where\n",
        "        # `log_every_n_steps` is a parameter to the `Trainer` object\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False)\n",
        "        self.train_metrics(y_hat[:, 1], y)\n",
        "\n",
        "        return cast(Tensor, loss)\n",
        "\n",
        "    def training_epoch_end(self, outputs: Any) -> None:\n",
        "        \"\"\"Logs epoch-level training metrics.\n",
        "        Args:\n",
        "            outputs: list of items returned by training_step\n",
        "        \"\"\"\n",
        "        self.log_dict(self.train_metrics.compute())\n",
        "        self.train_metrics.reset()\n",
        "\n",
        "    def validation_step(self, *args: Any, **kwargs: Any) -> None:\n",
        "        \"\"\"Compute validation loss and log example predictions.\n",
        "        Args:\n",
        "            batch: the output of your DataLoader\n",
        "            batch_idx: the index of this batch\n",
        "        \"\"\"\n",
        "        batch = args[0]\n",
        "        batch_idx = args[1]\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.val_metrics(y_hat[:, 1], y)\n",
        "\n",
        "        if (\n",
        "            batch_idx < 10\n",
        "            and hasattr(self.trainer, \"datamodule\")\n",
        "            and self.logger\n",
        "            and hasattr(self.logger, \"experiment\")\n",
        "        ):\n",
        "            try:\n",
        "                pred = y_hat_hard\n",
        "                for key in x, y, pred:\n",
        "                    key = key.cpu()\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "    def validation_epoch_end(self, outputs: Any) -> None:\n",
        "        \"\"\"Logs epoch level validation metrics.\n",
        "        Args:\n",
        "            outputs: list of items returned by validation_step\n",
        "        \"\"\"\n",
        "        self.log_dict(self.val_metrics.compute())\n",
        "        self.val_metrics.reset()\n",
        "\n",
        "    def test_step(self, *args: Any, **kwargs: Any) -> None:\n",
        "        \"\"\"Compute test loss.\n",
        "        Args:\n",
        "            batch: the output of your DataLoader\n",
        "        \"\"\"\n",
        "        batch = args[0]\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        y_hat_hard = y_hat.argmax(dim=1)\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "\n",
        "        # by default, the test and validation steps only log per *epoch*\n",
        "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
        "        self.test_metrics(y_hat[:, 1], y)\n",
        "\n",
        "    def test_epoch_end(self, outputs: Any) -> None:\n",
        "        \"\"\"Logs epoch level test metrics.\n",
        "        Args:\n",
        "            outputs: list of items returned by test_step\n",
        "        \"\"\"\n",
        "        self.log_dict(self.test_metrics.compute())\n",
        "        self.test_metrics.reset()\n",
        "\n",
        "    def predict_step(self, *args: Any, **kwargs: Any) -> Tensor:\n",
        "        \"\"\"Compute and return the predictions.\n",
        "        Args:\n",
        "            batch: the output of your DataLoader\n",
        "        Returns:\n",
        "            predicted softmax probabilities\n",
        "        \"\"\"\n",
        "        batch = args[0]\n",
        "        x, y = batch\n",
        "        y_hat: Tensor = self(x).softmax(dim=-1)\n",
        "        return y_hat\n",
        "\n",
        "    def configure_optimizers(self) -> Dict[str, Any]:\n",
        "        \"\"\"Initialize the optimizer and learning rate scheduler.\n",
        "        Returns:\n",
        "            a \"lr dict\" according to the pytorch lightning documentation --\n",
        "            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n",
        "        \"\"\"\n",
        "        if self.hyperparams[\"optimizer\"].lower() == \"adamw\":\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                self.model.parameters(),\n",
        "                lr=self.hyperparams[\"learning_rate\"],\n",
        "                weight_decay=self.hyperparams[\"weight_decay\"],\n",
        "            )\n",
        "        elif self.hyperparams[\"optimizer\"].lower() == \"sgd\":\n",
        "            optimizer = torch.optim.SGD(\n",
        "                self.model.parameters(),\n",
        "                lr=self.hyperparams[\"learning_rate\"],\n",
        "                weight_decay=self.hyperparams[\"weight_decay\"],\n",
        "            )\n",
        "        if self.hyperparams[\"lr_scheduler\"] == \"on_plateau\":\n",
        "          scheduler = ReduceLROnPlateau(\n",
        "                    optimizer,\n",
        "                    patience=self.hyperparams[\"learning_rate_schedule_patience\"],\n",
        "                )\n",
        "        else:\n",
        "          scheduler = StepLR(\n",
        "              optimizer,\n",
        "              step_size=self.hyperparams[\"learning_rate_schedule_patience\"],\n",
        "          )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",\n",
        "            },\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWjvTewD7QPU"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3X7wj07v_18"
      },
      "outputs": [],
      "source": [
        "wandb_namespace = \"\"\n",
        "wandb_project_name = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5pGXLUlH0cv"
      },
      "outputs": [],
      "source": [
        "# sweep_id = wandb.sweep(sweep_config, entity=wandb_namespace, project=wandb_project_name)\n",
        "sweep_id = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMMWyl4D8RBn"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  config_defaults = {\n",
        "    \"epochs\": 100,\n",
        "    \"batch_size\": 32,\n",
        "    \"dropout\": 0.,\n",
        "    \"weight_decay\": 0,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"optimizer\": \"adamw\",\n",
        "    \"model_name\": \"resnet50\",\n",
        "    \"weights\": \"imagenet\",\n",
        "    \"num_layers_to_finetune\": None,\n",
        "    \"learning_rate_schedule_patience\": 5,\n",
        "    \"early_stop_patience\": 10,\n",
        "    \"normalize\": False,\n",
        "    \"base_size\": 64, \n",
        "    \"dro\": None, \n",
        "    \"lr_scheduler\": \"on_plateau\",\n",
        "  }\n",
        "  # Initialize a new wandb run\n",
        "  run = wandb.init(config=config_defaults, entity=wandb_namespace, project=wandb_project_name)\n",
        "  wandb_logger = WandbLogger(\n",
        "      entity=wandb_namespace, project=wandb_project_name, run_id=run.id,\n",
        "      # log_model=\"all\"\n",
        "  )\n",
        "\n",
        "\n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "\n",
        "  base_size = config.base_size if config.model_name != \"vit_small_patch16_224\" else 224\n",
        "  data = DataModule(\n",
        "      data_dir=\"/content/minefree-class-128\",\n",
        "      batch_size=config.batch_size,\n",
        "      normalize=config.normalize,\n",
        "      dro=config.dro,\n",
        "      base_size=base_size\n",
        "  )\n",
        "  data.setup()\n",
        "\n",
        "  if config.model_name == \"resnet18\":\n",
        "      if config.weights is None or config.weights == \"sentinel2\":\n",
        "          weights = \"ResNet18_Weights.SENTINEL2_RGB_SECO\"\n",
        "\n",
        "  elif config.model_name == \"resnet50\":\n",
        "      if config.weights is None or config.weights == \"sentinel2\":\n",
        "          weights = \"ResNet50_Weights.SENTINEL2_RGB_SECO\"\n",
        "\n",
        "  elif config.model_name == \"vit_small_patch16_224\":\n",
        "      if config.weights is None or config.weights == \"sentinel2\":\n",
        "          weights = \"ViTSmall16_Weights.SENTINEL2_ALL_SECO\"\n",
        "\n",
        "  task = ClassificationTask(\n",
        "      model=config.model_name,\n",
        "      weights=config.weights,\n",
        "      loss=\"ce\",\n",
        "      in_channels=3,\n",
        "      num_classes=2,\n",
        "      batch_size=config.batch_size,\n",
        "      learning_rate=config.learning_rate,\n",
        "      learning_rate_schedule_patience=config.learning_rate_schedule_patience,\n",
        "      num_layers_to_finetune=config.num_layers_to_finetune,\n",
        "      optimizer=config.optimizer,\n",
        "      weight_decay=config.weight_decay,\n",
        "      dropout=config.dropout,\n",
        "      lr_scheduler=config.lr_scheduler\n",
        "  )\n",
        "\n",
        "  early_stop_callback = EarlyStopping(\n",
        "        monitor='val_AUROC',\n",
        "        patience=config.early_stop_patience,\n",
        "        verbose=False,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "  task.config_model()\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "    callbacks=[early_stop_callback],\n",
        "    logger=wandb_logger,\n",
        "    log_every_n_steps=5,\n",
        "    gpus=-1,\n",
        "    max_epochs=config.epochs,\n",
        "  )\n",
        "\n",
        "  trainer.fit(task, data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vc_oVWrGpS-"
      },
      "source": [
        "## Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-Y6SaAjMIS4"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train,\n",
        "            entity=wandb_namespace, project=wandb_project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIzym5dyLAfZ"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}